{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I2XpUHAjuJG5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoonieJang/CS481/blob/main/HW_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg3K0n2-twiV"
      },
      "source": [
        "# Homework 2\n",
        "\n",
        "- Please study lec_05, lec_06, lec_07, and lec_08 for this homework\n",
        "- There are three parts in the homework: <br>\n",
        "    > Part 1. practice encode, decode, and unicode <br>\n",
        "    > Part 2. practice using regular expression to extract information <br>\n",
        "    > Part 3. practice Part-of-Speech tagging <br>\n",
        "- The **deadline** is: 2021.03.06 10:00pm\n",
        "\n",
        "- Please run your code in Google Colab to avoid unnecessary confusion and the expected outputs are shown below each code cell.\n",
        "  >  Write your code in the code cell commented as \"write your code here / fill in the function\" <br>\n",
        "  > Please do not remove the comment \"write your code here / fill in the function\" <br>\n",
        "  > Please do not change variable names <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5ZK2mARs8L4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec595eb-0c41-4843-a3a9-1190a3a81579"
      },
      "source": [
        "import nltk\n",
        "nltk.download('book') # download the data used in nltk book"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2XpUHAjuJG5"
      },
      "source": [
        "## Part 1: practice encoding, decoding, and unicode "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oXJXb8kxcgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802ab587-7444-41f3-bdec-223f333b1526"
      },
      "source": [
        "# read the polish text file\n",
        "file = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
        "\n",
        "f = open(file, encoding='latin2') # tell the function that this file is encoded with latin2\n",
        "for line in f:\n",
        "    line = line.strip() # remove the leading and the trailing characters (e.g., space, \\n)\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
            "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
            "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
            "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
            "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
            "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Fm58LEudIv"
      },
      "source": [
        "- check the unicode representation of each line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh2jy5wZ4_pI",
        "outputId": "70a9b6d3-e8a0-4249-a269-604a0de901fb"
      },
      "source": [
        "# write your code here\n",
        "f = open(file, encoding='latin2')\n",
        "for line in f:\n",
        "    line = line.strip()\n",
        "    print(line.encode('unicode_escape'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
            "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
            "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
            "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
            "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
            "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTeT41F5ukPa"
      },
      "source": [
        "- Print letters that are not in the ASCII range\n",
        "    - hint: ord(the letter) > 127\n",
        "    - print the following information of the letters:\n",
        "        - letter \n",
        "        - the byte representation of the letter encoded with 'latin2'\n",
        "        - the byte representation of the letter encoded with 'ISO-8859-2'\n",
        "        - the byte representation of the letter encoded with 'utf8'\n",
        "        - the Unicode representation\n",
        "        - the name assigned to this letter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnE2vOSVugx5",
        "outputId": "75b83a57-4c0c-4346-a91b-c1bd84a82219"
      },
      "source": [
        "# write your code here\n",
        "import unicodedata\n",
        "lines = open(file, encoding='latin2').readlines()\n",
        "for i in lines:\n",
        "  for c in i: # iterate over character\n",
        "      if ord(c) > 127: # text outside the ASCII range\n",
        "          print('{} {} {} {} U+{:04x} -> {}'.format(c, c.encode('latin2'),c.encode('ISO-8859-2'),c.encode('utf8'), ord(c), unicodedata.name(c))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ń b'\\xf1' b'\\xf1' b'\\xc5\\x84' U+0144 -> LATIN SMALL LETTER N WITH ACUTE\n",
            "ą b'\\xb1' b'\\xb1' b'\\xc4\\x85' U+0105 -> LATIN SMALL LETTER A WITH OGONEK\n",
            "ó b'\\xf3' b'\\xf3' b'\\xc3\\xb3' U+00f3 -> LATIN SMALL LETTER O WITH ACUTE\n",
            "ś b'\\xb6' b'\\xb6' b'\\xc5\\x9b' U+015b -> LATIN SMALL LETTER S WITH ACUTE\n",
            "Ś b'\\xa6' b'\\xa6' b'\\xc5\\x9a' U+015a -> LATIN CAPITAL LETTER S WITH ACUTE\n",
            "ą b'\\xb1' b'\\xb1' b'\\xc4\\x85' U+0105 -> LATIN SMALL LETTER A WITH OGONEK\n",
            "ł b'\\xb3' b'\\xb3' b'\\xc5\\x82' U+0142 -> LATIN SMALL LETTER L WITH STROKE\n",
            "ł b'\\xb3' b'\\xb3' b'\\xc5\\x82' U+0142 -> LATIN SMALL LETTER L WITH STROKE\n",
            "ń b'\\xf1' b'\\xf1' b'\\xc5\\x84' U+0144 -> LATIN SMALL LETTER N WITH ACUTE\n",
            "ą b'\\xb1' b'\\xb1' b'\\xc4\\x85' U+0105 -> LATIN SMALL LETTER A WITH OGONEK\n",
            "ó b'\\xf3' b'\\xf3' b'\\xc3\\xb3' U+00f3 -> LATIN SMALL LETTER O WITH ACUTE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH6gdyASutIB"
      },
      "source": [
        "- Extract words consisting non-alphabet letters\n",
        "    - non-alphabet letters such as: ń,ó\n",
        "    - words consisting non-alphabet letters such as: państwowa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIav9MKNutqf",
        "outputId": "b960772c-10bf-49a2-dcfe-a14f38700213"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "special_letters = [ ]  # define the list of special letters from the previous output \n",
        "# write your code here\n",
        "# note: \"śląsk\" appears twice because it contains two special characters \"ś\" and \"ą\", it's fine if you want to remove duplicated words.\n",
        "import re\n",
        "\n",
        "for i in f_lines:\n",
        "  for c in i: # iterate over character\n",
        "      if ord(c) > 127:\n",
        "        special_letters.append(c)\n",
        "\n",
        "f = open(file, encoding='latin2') # tell the function that this file is encoded with latin2\n",
        "tokens=[]\n",
        "for line in f:\n",
        "    token = nltk.word_tokenize(line)\n",
        "    for i in token:\n",
        "      tokens.append(i)\n",
        "tok=[]     \n",
        "for b in tokens:\n",
        "    for a in special_letters:\n",
        "        if a in b:\n",
        "          tok.append(b)\n",
        "set(tok)\n",
        "# write your code here\n",
        "# note: \"śląsk\" appears twice because it contains two special characters \"ś\" and \"ą\", it's fine if you want to remove duplicated words.\n",
        "# write your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Jagiellońskiej',\n",
              " 'Niemców',\n",
              " 'Państwowa',\n",
              " 'Trafiły',\n",
              " 'archiwaliów',\n",
              " 'nazwą',\n",
              " 'obejmują',\n",
              " 'zostały',\n",
              " 'Śląsk',\n",
              " 'światowej'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yux38F-fu9Qy"
      },
      "source": [
        "## Part 2: access a webpage and extract required information using regular expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfIMl-mfuyBL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78365ec-aad0-4a2d-fa69-6b0eb7cf2aa0"
      },
      "source": [
        "# get contents from a URL\n",
        "from urllib import request\n",
        "html = request.urlopen('http://nltk.org/').read().decode('utf8') \n",
        "print(html)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "<!DOCTYPE html>\n",
            "\n",
            "<html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
            "  <head>\n",
            "    <meta charset=\"utf-8\" />\n",
            "    <title>Natural Language Toolkit &#8212; NLTK 3.5 documentation</title>\n",
            "    <link rel=\"stylesheet\" href=\"_static/agogo.css\" type=\"text/css\" />\n",
            "    <link rel=\"stylesheet\" href=\"_static/pygments.css\" type=\"text/css\" />\n",
            "    <script id=\"documentation_options\" data-url_root=\"./\" src=\"_static/documentation_options.js\"></script>\n",
            "    <script src=\"_static/jquery.js\"></script>\n",
            "    <script src=\"_static/underscore.js\"></script>\n",
            "    <script src=\"_static/doctools.js\"></script>\n",
            "    <script src=\"_static/language_data.js\"></script>\n",
            "    <link rel=\"index\" title=\"Index\" href=\"genindex.html\" />\n",
            "    <link rel=\"search\" title=\"Search\" href=\"search.html\" />\n",
            "    <link rel=\"next\" title=\"NLTK News\" href=\"news.html\" /> \n",
            "  </head><body>\n",
            "    <div class=\"header-wrapper\" role=\"banner\">\n",
            "      <div class=\"header\">\n",
            "        <div class=\"headertitle\"><a\n",
            "          href=\"#\">NLTK 3.5 documentation</a></div>\n",
            "        <div class=\"rel\" role=\"navigation\" aria-label=\"related navigation\">\n",
            "          <a href=\"news.html\" title=\"NLTK News\"\n",
            "             accesskey=\"N\">next</a> |\n",
            "          <a href=\"py-modindex.html\" title=\"Python Module Index\"\n",
            "             >modules</a> |\n",
            "          <a href=\"genindex.html\" title=\"General Index\"\n",
            "             accesskey=\"I\">index</a>\n",
            "        </div>\n",
            "       </div>\n",
            "    </div>\n",
            "\n",
            "    <div class=\"content-wrapper\">\n",
            "      <div class=\"content\">\n",
            "        <div class=\"document\">\n",
            "            \n",
            "      <div class=\"documentwrapper\">\n",
            "        <div class=\"bodywrapper\">\n",
            "          <div class=\"body\" role=\"main\">\n",
            "            \n",
            "  <div class=\"section\" id=\"natural-language-toolkit\">\n",
            "<h1>Natural Language Toolkit<a class=\"headerlink\" href=\"#natural-language-toolkit\" title=\"Permalink to this headline\">¶</a></h1>\n",
            "<p>NLTK is a leading platform for building Python programs to work with human language data.\n",
            "It provides easy-to-use interfaces to <a class=\"reference external\" href=\"http://nltk.org/nltk_data/\">over 50 corpora and lexical\n",
            "resources</a> such as WordNet,\n",
            "along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\n",
            "wrappers for industrial-strength NLP libraries,\n",
            "and an active <a class=\"reference external\" href=\"http://groups.google.com/group/nltk-users\">discussion forum</a>.</p>\n",
            "<p>Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\n",
            "NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\n",
            "NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.</p>\n",
            "<p>NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\n",
            "and “an amazing library to play with natural language.”</p>\n",
            "<p><a class=\"reference external\" href=\"http://nltk.org/book\">Natural Language Processing with Python</a> provides a practical\n",
            "introduction to programming for language processing.\n",
            "Written by the creators of NLTK, it guides the reader through the fundamentals\n",
            "of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\n",
            "and more.\n",
            "The online version of the book has been been updated for Python 3 and NLTK 3.\n",
            "(The original Python 2 version is still available at <a class=\"reference external\" href=\"http://nltk.org/book_1ed\">http://nltk.org/book_1ed</a>.)</p>\n",
            "<div class=\"section\" id=\"some-simple-things-you-can-do-with-nltk\">\n",
            "<h2>Some simple things you can do with NLTK<a class=\"headerlink\" href=\"#some-simple-things-you-can-do-with-nltk\" title=\"Permalink to this headline\">¶</a></h2>\n",
            "<p>Tokenize and tag some text:</p>\n",
            "<div class=\"doctest highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n",
            "<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">sentence</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;&quot;&quot;At eight o&#39;clock on Thursday morning</span>\n",
            "<span class=\"gp\">... </span><span class=\"s2\">Arthur didn&#39;t feel very good.&quot;&quot;&quot;</span>\n",
            "<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">word_tokenize</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n",
            "<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">tokens</span>\n",
            "<span class=\"go\">[&#39;At&#39;, &#39;eight&#39;, &quot;o&#39;clock&quot;, &#39;on&#39;, &#39;Thursday&#39;, &#39;morning&#39;,</span>\n",
            "<span class=\"go\">&#39;Arthur&#39;, &#39;did&#39;, &quot;n&#39;t&quot;, &#39;feel&#39;, &#39;very&#39;, &#39;good&#39;, &#39;.&#39;]</span>\n",
            "<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">tagged</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">pos_tag</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n",
            "<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">tagged</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">6</span><span class=\"p\">]</span>\n",
            "<span class=\"go\">[(&#39;At&#39;, &#39;IN&#39;), (&#39;eight&#39;, &#39;CD&#39;), (&quot;o&#39;clock&quot;, &#39;JJ&#39;), (&#39;on&#39;, &#39;IN&#39;),</span>\n",
            "<span class=\"go\">(&#39;Thursday&#39;, &#39;NNP&#39;), (&#39;morning&#39;, &#39;NN&#39;)]</span>\n",
            "</pre></div>\n",
            "</div>\n",
            "<p>Identify named entities:</p>\n",
            "<div class=\"doctest highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">entities</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">chunk</span><span class=\"o\">.</span><span class=\"n\">ne_chunk</span><span class=\"p\">(</span><span class=\"n\">tagged</span><span class=\"p\">)</span>\n",
            "<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">entities</span>\n",
            "<span class=\"go\">Tree(&#39;S&#39;, [(&#39;At&#39;, &#39;IN&#39;), (&#39;eight&#39;, &#39;CD&#39;), (&quot;o&#39;clock&quot;, &#39;JJ&#39;),</span>\n",
            "<span class=\"go\">           (&#39;on&#39;, &#39;IN&#39;), (&#39;Thursday&#39;, &#39;NNP&#39;), (&#39;morning&#39;, &#39;NN&#39;),</span>\n",
            "<span class=\"go\">       Tree(&#39;PERSON&#39;, [(&#39;Arthur&#39;, &#39;NNP&#39;)]),</span>\n",
            "<span class=\"go\">           (&#39;did&#39;, &#39;VBD&#39;), (&quot;n&#39;t&quot;, &#39;RB&#39;), (&#39;feel&#39;, &#39;VB&#39;),</span>\n",
            "<span class=\"go\">           (&#39;very&#39;, &#39;RB&#39;), (&#39;good&#39;, &#39;JJ&#39;), (&#39;.&#39;, &#39;.&#39;)])</span>\n",
            "</pre></div>\n",
            "</div>\n",
            "<p>Display a parse tree:</p>\n",
            "<div class=\"doctest highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">nltk.corpus</span> <span class=\"kn\">import</span> <span class=\"n\">treebank</span>\n",
            "<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">treebank</span><span class=\"o\">.</span><span class=\"n\">parsed_sents</span><span class=\"p\">(</span><span class=\"s1\">&#39;wsj_0001.mrg&#39;</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
            "<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">draw</span><span class=\"p\">()</span>\n",
            "</pre></div>\n",
            "</div>\n",
            "<img alt=\"_images/tree.gif\" src=\"_images/tree.gif\" />\n",
            "<p>NB. If you publish work that uses NLTK, please cite the NLTK book as\n",
            "follows:</p>\n",
            "<blockquote>\n",
            "<div><p>Bird, Steven, Edward Loper and Ewan Klein (2009), <em>Natural Language Processing with Python</em>.  O’Reilly Media Inc.</p>\n",
            "</div></blockquote>\n",
            "</div>\n",
            "<div class=\"section\" id=\"next-steps\">\n",
            "<h2>Next Steps<a class=\"headerlink\" href=\"#next-steps\" title=\"Permalink to this headline\">¶</a></h2>\n",
            "<ul class=\"simple\">\n",
            "<li><p><a class=\"reference external\" href=\"http://groups.google.com/group/nltk\">sign up for release announcements</a></p></li>\n",
            "<li><p><a class=\"reference external\" href=\"http://groups.google.com/group/nltk-users\">join in the discussion</a></p></li>\n",
            "</ul>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"section\" id=\"contents\">\n",
            "<h1>Contents<a class=\"headerlink\" href=\"#contents\" title=\"Permalink to this headline\">¶</a></h1>\n",
            "<div class=\"toctree-wrapper compound\">\n",
            "<ul>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"news.html\">NLTK News</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"install.html\">Installing NLTK</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"data.html\">Installing NLTK Data</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"contribute.html\">Contribute to NLTK</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://github.com/nltk/nltk/wiki/FAQ\">FAQ</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://github.com/nltk/nltk/wiki\">Wiki</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/nltk.html\">API</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference external\" href=\"http://www.nltk.org/howto\">HOWTO</a></li>\n",
            "</ul>\n",
            "</div>\n",
            "<ul class=\"simple\">\n",
            "<li><p><a class=\"reference internal\" href=\"genindex.html\"><span class=\"std std-ref\">Index</span></a></p></li>\n",
            "<li><p><a class=\"reference internal\" href=\"py-modindex.html\"><span class=\"std std-ref\">Module Index</span></a></p></li>\n",
            "<li><p><a class=\"reference internal\" href=\"search.html\"><span class=\"std std-ref\">Search Page</span></a></p></li>\n",
            "</ul>\n",
            "</div>\n",
            "\n",
            "\n",
            "          </div>\n",
            "        </div>\n",
            "      </div>\n",
            "        </div>\n",
            "        <div class=\"sidebar\">\n",
            "          \n",
            "          <h3>Table of Contents</h3>\n",
            "          <ul>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"news.html\">NLTK News</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"install.html\">Installing NLTK</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"data.html\">Installing NLTK Data</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"contribute.html\">Contribute to NLTK</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://github.com/nltk/nltk/wiki/FAQ\">FAQ</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://github.com/nltk/nltk/wiki\">Wiki</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/nltk.html\">API</a></li>\n",
            "<li class=\"toctree-l1\"><a class=\"reference external\" href=\"http://www.nltk.org/howto\">HOWTO</a></li>\n",
            "</ul>\n",
            "\n",
            "          <div role=\"search\">\n",
            "            <h3 style=\"margin-top: 1.5em;\">Search</h3>\n",
            "            <form class=\"search\" action=\"search.html\" method=\"get\">\n",
            "                <input type=\"text\" name=\"q\" />\n",
            "                <input type=\"submit\" value=\"Go\" />\n",
            "            </form>\n",
            "          </div>\n",
            "\n",
            "        </div>\n",
            "        <div class=\"clearer\"></div>\n",
            "      </div>\n",
            "    </div>\n",
            "\n",
            "    <div class=\"footer-wrapper\">\n",
            "      <div class=\"footer\">\n",
            "        <div class=\"left\">\n",
            "          <div role=\"navigation\" aria-label=\"related navigaton\">\n",
            "            <a href=\"news.html\" title=\"NLTK News\"\n",
            "              >next</a> |\n",
            "            <a href=\"py-modindex.html\" title=\"Python Module Index\"\n",
            "              >modules</a> |\n",
            "            <a href=\"genindex.html\" title=\"General Index\"\n",
            "              >index</a>\n",
            "          </div>\n",
            "          <div role=\"note\" aria-label=\"source link\">\n",
            "              <br/>\n",
            "              <a href=\"_sources/index.rst.txt\"\n",
            "                rel=\"nofollow\">Show Source</a>\n",
            "          </div>\n",
            "        </div>\n",
            "\n",
            "        <div class=\"right\">\n",
            "          \n",
            "    <div class=\"footer\" role=\"contentinfo\">\n",
            "        &#169; Copyright 2020, NLTK Project.\n",
            "      Last updated on Apr 13, 2020.\n",
            "      Created using <a href=\"http://sphinx-doc.org/\">Sphinx</a> 2.4.4.\n",
            "    </div>\n",
            "        </div>\n",
            "        <div class=\"clearer\"></div>\n",
            "      </div>\n",
            "    </div>\n",
            "\n",
            "  </body>\n",
            "</html>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkC7QGStvCmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29cca3d9-9127-42bb-d94c-3f7e7506d8a9"
      },
      "source": [
        "# use BeautifulSoup to pull text data from the HTML file\n",
        "from bs4 import BeautifulSoup\n",
        "raw_text =  BeautifulSoup(html,'html.parser').get_text() # write your code here\n",
        "print(raw_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Natural Language Toolkit — NLTK 3.5 documentation\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NLTK 3.5 documentation\n",
            "\n",
            "next |\n",
            "          modules |\n",
            "          index\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Natural Language Toolkit¶\n",
            "NLTK is a leading platform for building Python programs to work with human language data.\n",
            "It provides easy-to-use interfaces to over 50 corpora and lexical\n",
            "resources such as WordNet,\n",
            "along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\n",
            "wrappers for industrial-strength NLP libraries,\n",
            "and an active discussion forum.\n",
            "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\n",
            "NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\n",
            "NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
            "NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\n",
            "and “an amazing library to play with natural language.”\n",
            "Natural Language Processing with Python provides a practical\n",
            "introduction to programming for language processing.\n",
            "Written by the creators of NLTK, it guides the reader through the fundamentals\n",
            "of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\n",
            "and more.\n",
            "The online version of the book has been been updated for Python 3 and NLTK 3.\n",
            "(The original Python 2 version is still available at http://nltk.org/book_1ed.)\n",
            "\n",
            "Some simple things you can do with NLTK¶\n",
            "Tokenize and tag some text:\n",
            ">>> import nltk\n",
            ">>> sentence = \"\"\"At eight o'clock on Thursday morning\n",
            "... Arthur didn't feel very good.\"\"\"\n",
            ">>> tokens = nltk.word_tokenize(sentence)\n",
            ">>> tokens\n",
            "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n",
            "'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
            ">>> tagged = nltk.pos_tag(tokens)\n",
            ">>> tagged[0:6]\n",
            "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n",
            "('Thursday', 'NNP'), ('morning', 'NN')]\n",
            "\n",
            "\n",
            "Identify named entities:\n",
            ">>> entities = nltk.chunk.ne_chunk(tagged)\n",
            ">>> entities\n",
            "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'),\n",
            "           ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),\n",
            "       Tree('PERSON', [('Arthur', 'NNP')]),\n",
            "           ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'),\n",
            "           ('very', 'RB'), ('good', 'JJ'), ('.', '.')])\n",
            "\n",
            "\n",
            "Display a parse tree:\n",
            ">>> from nltk.corpus import treebank\n",
            ">>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
            ">>> t.draw()\n",
            "\n",
            "\n",
            "\n",
            "NB. If you publish work that uses NLTK, please cite the NLTK book as\n",
            "follows:\n",
            "\n",
            "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\n",
            "\n",
            "\n",
            "\n",
            "Next Steps¶\n",
            "\n",
            "sign up for release announcements\n",
            "join in the discussion\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Contents¶\n",
            "\n",
            "\n",
            "NLTK News\n",
            "Installing NLTK\n",
            "Installing NLTK Data\n",
            "Contribute to NLTK\n",
            "FAQ\n",
            "Wiki\n",
            "API\n",
            "HOWTO\n",
            "\n",
            "\n",
            "\n",
            "Index\n",
            "Module Index\n",
            "Search Page\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Table of Contents\n",
            "\n",
            "NLTK News\n",
            "Installing NLTK\n",
            "Installing NLTK Data\n",
            "Contribute to NLTK\n",
            "FAQ\n",
            "Wiki\n",
            "API\n",
            "HOWTO\n",
            "\n",
            "\n",
            "Search\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "next |\n",
            "            modules |\n",
            "            index\n",
            "\n",
            "\n",
            "\n",
            "Show Source\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "        © Copyright 2020, NLTK Project.\n",
            "      Last updated on Apr 13, 2020.\n",
            "      Created using Sphinx 2.4.4.\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOyG2_zbvLgP"
      },
      "source": [
        "- Write a regular expression to extract **digits** from the text\n",
        "    - the order of elements in the output list doesn't matter\n",
        "\n",
        "- hints: \n",
        "    - re.findall()\n",
        "    - think carefully about the patterns of these digits\n",
        "        - one or more digits\n",
        "        - digits separated by dot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H8rQDIDvGUt"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLDhivjsvPlF",
        "outputId": "579ca502-550a-4b1d-a18d-4ba07feef6ad"
      },
      "source": [
        "# write your code here\n",
        "re.findall(r'\\d+\\.+\\d+\\d*\\.*\\d*|\\d+', raw_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3.5',\n",
              " '3.5',\n",
              " '50',\n",
              " '3',\n",
              " '3',\n",
              " '2',\n",
              " '1',\n",
              " '0',\n",
              " '6',\n",
              " '0001',\n",
              " '0',\n",
              " '2009',\n",
              " '2020',\n",
              " '13',\n",
              " '2020',\n",
              " '2.4.4']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDnxQqNCvT2G"
      },
      "source": [
        "- Write a regular expression to extract **upper-cased sequence of letters** from the text\n",
        "    - all letters in the sequence are in upper-case format\n",
        "    - the sequence should have at least two upper-case letters\n",
        "    - use set() function to remove repeated elements\n",
        "    - the order of elements in the output doesn't matter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtYbG-oSvRAg",
        "outputId": "09815830-2ce5-4549-af8b-601dbb506df2"
      },
      "source": [
        "# write your code here\n",
        "set(re.findall(r'[A-Z][A-Z]+',raw_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'API',\n",
              " 'CD',\n",
              " 'FAQ',\n",
              " 'HOWTO',\n",
              " 'IN',\n",
              " 'JJ',\n",
              " 'NB',\n",
              " 'NLP',\n",
              " 'NLTK',\n",
              " 'NN',\n",
              " 'NNP',\n",
              " 'OS',\n",
              " 'PERSON',\n",
              " 'RB',\n",
              " 'VB',\n",
              " 'VBD'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJfFRE86vYU9"
      },
      "source": [
        "- Write a regular expression to **substitute** 'NLP' to 'Natural Language Processing' in the given sentence\n",
        "    - re.sub()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Uttx5NWBvWAk",
        "outputId": "2bac8c3f-5109-4b75-e335-2f5b21f0ea77"
      },
      "source": [
        "sentence = 'NLTK provides wrappers for industrial-strength NLP libraries'\n",
        "re.sub(r'NLP', r'Natural Language Processing',sentence) # fill the function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NLTK provides wrappers for industrial-strength Natural Language Processing libraries'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acX3ZYENvc2r"
      },
      "source": [
        "- Write a regular expression to find **words ending in ing**\n",
        "    - use set() function to remove repeated elements\n",
        "    - the order of words in the output doesn't matter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPlHur1Rvaal",
        "outputId": "e5b367de-a833-4616-aac8-cd0932c30e35"
      },
      "source": [
        "# write your code here\n",
        "set( re.findall('\\w+ing', raw_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Installing',\n",
              " 'Processing',\n",
              " 'amazing',\n",
              " 'analyzing',\n",
              " 'building',\n",
              " 'categorizing',\n",
              " 'introducing',\n",
              " 'leading',\n",
              " 'ling',\n",
              " 'morning',\n",
              " 'parsing',\n",
              " 'processing',\n",
              " 'programming',\n",
              " 'reasoning',\n",
              " 'stemming',\n",
              " 'tagging',\n",
              " 'teaching',\n",
              " 'thing',\n",
              " 'using',\n",
              " 'working',\n",
              " 'writing'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLsjbQghviGm"
      },
      "source": [
        "- Write a regular expression to **extract the url**\n",
        "    - re.search()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLCoUHZEvfcq",
        "outputId": "c71f5efb-6333-40bf-9024-6119339feec7"
      },
      "source": [
        "# write your code here\n",
        "re.search(r'(http|https)://\\w+..+d',raw_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(1541, 1565), match='http://nltk.org/book_1ed'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uOVoZqNvpIe"
      },
      "source": [
        "## Part 3: practice POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybS66e0_vlwN"
      },
      "source": [
        "text = \"John's big idea isn't all that bad\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-z5jL1R2R8z"
      },
      "source": [
        "- For the given sentence, check the POS tag with tagset=None"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WKOzbWbvst_",
        "outputId": "b11250f8-606d-43bd-aad9-4e8cc659a5f9"
      },
      "source": [
        "# write your code here\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text =word_tokenize(\"John's big idea isn't all that bad\")\n",
        "nltk.pos_tag(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NNP'),\n",
              " (\"'s\", 'POS'),\n",
              " ('big', 'JJ'),\n",
              " ('idea', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " (\"n't\", 'RB'),\n",
              " ('all', 'PDT'),\n",
              " ('that', 'DT'),\n",
              " ('bad', 'JJ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IvOpgVZvxy_"
      },
      "source": [
        "- Check the POS tag for the given sentence with tagset = 'universal'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEuGAzSHvuWl",
        "outputId": "1e6dbe55-2485-4fca-a011-a5ee99a53fe9"
      },
      "source": [
        "# write your code here\n",
        "nltk.download('universal_tagset')\n",
        "nltk.pos_tag(text,tagset = 'universal')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NOUN'),\n",
              " (\"'s\", 'PRT'),\n",
              " ('big', 'ADJ'),\n",
              " ('idea', 'NOUN'),\n",
              " ('is', 'VERB'),\n",
              " (\"n't\", 'ADV'),\n",
              " ('all', 'DET'),\n",
              " ('that', 'DET'),\n",
              " ('bad', 'ADJ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMq0GJH4wCOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b13eb0f-ed39-49ee-a518-ab6630e823ad"
      },
      "source": [
        "# check the specific meaning of each POS tag in the Penn Treebank tagset\n",
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbw2GxNWv9D8"
      },
      "source": [
        "- Specify the training data and test data\n",
        "    - training data: brown corpus with news category\n",
        "    - testing data: bron corpus with humor category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFGMSe-zwJry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4af962-b1a1-4478-cf52-3a9d2455785d"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4lb4wIWv9dm",
        "outputId": "94620cc5-9f77-4dc8-b4d6-d2b9c103a03e"
      },
      "source": [
        "# write your code here\n",
        "brown_tagged_sents = brown.tagged_sents\n",
        "train_sents = brown_tagged_sents(categories='news')\n",
        "test_sents = brown_tagged_sents(categories='humor')\n",
        "print(\"%d training sentences, %d testing sentences\" % (len(train_sents), len(test_sents)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4623 training sentences, 1053 testing sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX2FyGHCwQPU"
      },
      "source": [
        "- train a default tagger using the news category of brown corpus\n",
        "    - hint: \n",
        "        - find the most frequent tag of the news category\n",
        "        - train a DefaultTagger with the most frequent tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LgAUIAYwqFm"
      },
      "source": [
        "- Get the most frequent tag for the brown corpus training data (categories = 'news')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JETdvwAnv9gr",
        "outputId": "826c8952-45b7-4996-e086-cbe6cbff37dc"
      },
      "source": [
        "# write your code here\n",
        "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
        "nltk.FreqDist(tags).max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e9CvAUNwvj-"
      },
      "source": [
        "- Train a DefaultTagger with the most frequent tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci3lPWD-v9kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231030c3-2c1f-46ea-9bb3-695ab1cb6674"
      },
      "source": [
        "default_tagger = nltk.DefaultTagger('NN') # write your code here\n",
        "default_tagger"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DefaultTagger: tag=NN>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVwdxGTHw0dJ"
      },
      "source": [
        "- Apply the DefaultTagger to tag the given text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP5LHQawv9mi",
        "outputId": "63de9d36-1d10-4824-d2b3-62bbb19505ea"
      },
      "source": [
        "text = word_tokenize(\"John's big idea isn't all that bad\")\n",
        "default_tagger.tag(text) # fill in the tag function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NN'),\n",
              " (\"'s\", 'NN'),\n",
              " ('big', 'NN'),\n",
              " ('idea', 'NN'),\n",
              " ('is', 'NN'),\n",
              " (\"n't\", 'NN'),\n",
              " ('all', 'NN'),\n",
              " ('that', 'NN'),\n",
              " ('bad', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg0xFqK2w4vS"
      },
      "source": [
        "- Evaluate the DefaultTagger's performance on testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVxk-81Gw2bT",
        "outputId": "75bc625a-3698-4410-b964-6b6df35b41b1"
      },
      "source": [
        "default_tagger.evaluate(test_sents) # fill in the evaluate function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11832219405392948"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f2piKQNw9Mp"
      },
      "source": [
        "- Train a **UnigramTagger** with the training data (categories='news')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPyi9Pkww9sn"
      },
      "source": [
        "- Get the top-n most frequent words and the most frequent tag for each word (in the training data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5ppQuqyw62a"
      },
      "source": [
        "def topn_frequent_tags(topn):\n",
        "    \"\"\"\n",
        "    - get topn frequent words\n",
        "    - for each frequent word, get its most frequent tag\n",
        "    - return: dict{word}=tag \n",
        "    \"\"\"\n",
        "    # write your code here\n",
        "    fd = nltk.FreqDist(brown.words(categories='news')) # dict[word]=frequency\n",
        "    conditional_fd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news')) # dict of dict: dict[word] = {dict{tag}=freq}\n",
        "    most_freq_words = fd.most_common(topn) # list of (word, frequency)\n",
        "    most_freq_wordtags = dict((word, conditional_fd[word].max()) for (word, freq) in most_freq_words)\n",
        "    return most_freq_wordtags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pu2Vyf-yt5I"
      },
      "source": [
        "- The top-10 most frequent word and the corresponding tags in training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhX-FUWVxDZ1",
        "outputId": "f1479eb6-6fc5-4ab3-8aa0-04a863d5ba00"
      },
      "source": [
        "topn_frequent_tags(topn=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',': ',',\n",
              " '.': '.',\n",
              " 'The': 'AT',\n",
              " 'a': 'AT',\n",
              " 'and': 'CC',\n",
              " 'for': 'IN',\n",
              " 'in': 'IN',\n",
              " 'of': 'IN',\n",
              " 'the': 'AT',\n",
              " 'to': 'TO'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk2BcGVXxH5d"
      },
      "source": [
        "- Build a UnigramTagger with the top-100 most frequent word tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlJ3cBasxK8Z"
      },
      "source": [
        "unigram_tagger_top100 =nltk.UnigramTagger(model=topn_frequent_tags(100)) # write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkGrSPpCxPZM"
      },
      "source": [
        "- Evaluate the performance of the unigram_tagger_top100 on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-zRXqwZxMpt",
        "outputId": "0905aeab-4d7a-4a00-a0c3-e3b09609f701"
      },
      "source": [
        "unigram_tagger_top100.evaluate(test_sents)# write your code here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.491126987785204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBsLGmXyxU61"
      },
      "source": [
        "- Build a UnigramTagger with the top-1000 most frequent word tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEry_SJJxRcY"
      },
      "source": [
        "unigram_tagger_top1000 = nltk.UnigramTagger(model=topn_frequent_tags(1000)) # write your code here# write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbkXXD-ZxZis"
      },
      "source": [
        "- Evaluate the performance of the unigram_tagger_top1000 on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ikcKbUJxXCv",
        "outputId": "253c9d40-a69a-4daa-9c0a-c215d07ec8c5"
      },
      "source": [
        "# write your code here\n",
        "unigram_tagger_top1000.evaluate(test_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6397787508642544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq1aADPsxeDq"
      },
      "source": [
        "- Build a UnigramTagger with all the tagged training sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n8hbCfuxbxq"
      },
      "source": [
        "unigram_tagger = nltk.UnigramTagger(train_sents) # write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTFaumXHxixo"
      },
      "source": [
        "- Evaluate the performance of the unigram_tagger on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3az3G8T_xgmb",
        "outputId": "dbf13596-c8d4-43c9-866c-8229461a240a"
      },
      "source": [
        "# write your code here\n",
        "unigram_tagger.evaluate(test_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7951140815856188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN96_vPyxm6h"
      },
      "source": [
        "- Build a BigramTagger with all the tagged training sentences and evaluate its performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG8CsrUlxkuZ",
        "outputId": "6dcf131f-c47f-48cc-f675-e82611265d8c"
      },
      "source": [
        "t2 =  nltk.BigramTagger(train_sents) # write your code here\n",
        "t2.evaluate(test_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11684719981562572"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hah-Xjh-xr05"
      },
      "source": [
        "- Apply the t2 tagger on the given sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1wV0dNGxpc2",
        "outputId": "add56fe0-76be-4b94-d480-4d55e2c87bb8"
      },
      "source": [
        "t2.tag(text) # fill in the tag function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NP'),\n",
              " (\"'s\", None),\n",
              " ('big', None),\n",
              " ('idea', None),\n",
              " ('is', None),\n",
              " (\"n't\", None),\n",
              " ('all', None),\n",
              " ('that', None),\n",
              " ('bad', None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGxMa4XBxwsl"
      },
      "source": [
        "- Add the UnigramTagger as a backoff for the BigramTagger and evaluate its performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBwJSMaRxuKG",
        "outputId": "ab462315-871d-448e-de47-362210a9e60f"
      },
      "source": [
        "t2_t1 = nltk.BigramTagger(train_sents, backoff=unigram_tagger)# write your code here# write your code here\n",
        "t2_t1.evaluate(test_sents) # fill in the evaluate function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.805992164093109"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb-tTdiix1qZ"
      },
      "source": [
        "- Apply the t2_t1 tagger on the given sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFwWfHvCxzB-",
        "outputId": "b18f4f7f-6a95-446e-993e-757ca000ff74"
      },
      "source": [
        "t2_t1.tag(text) # fill in the tag function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NP'),\n",
              " (\"'s\", None),\n",
              " ('big', 'JJ'),\n",
              " ('idea', 'NN'),\n",
              " ('is', 'BEZ'),\n",
              " (\"n't\", None),\n",
              " ('all', 'ABN'),\n",
              " ('that', 'DT'),\n",
              " ('bad', 'JJ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBqmVWDfx8e5"
      },
      "source": [
        "- Add the DefaultTagger as a backoff for the BigramTagger and evaluate its performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taWOb5jmx3oX",
        "outputId": "5e5f802f-52a7-4fb9-d9d9-dac099e48983"
      },
      "source": [
        "t2_default =nltk.BigramTagger(train_sents, backoff=default_tagger)# write your code here# write your code here # write your code here\n",
        "t2_default.evaluate(test_sents) # fill in the evaluate function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7252823231159253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MteDkSnyBTk"
      },
      "source": [
        "- Apply the t2_default tagger on the given sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCEa7jHFx-jF",
        "outputId": "fe401b40-60cf-443d-f45a-75a39f72d317"
      },
      "source": [
        "t2_default.tag(text) # fill in the tag function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NP'),\n",
              " (\"'s\", 'NN'),\n",
              " ('big', 'JJ'),\n",
              " ('idea', 'NN'),\n",
              " ('is', 'BEZ'),\n",
              " (\"n't\", 'NN'),\n",
              " ('all', 'ABN'),\n",
              " ('that', 'DT'),\n",
              " ('bad', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRQSvRWtyK-Q"
      },
      "source": [
        "- Summarize the performance of your taggers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmj829nMyDwl",
        "outputId": "541a1e5f-c5fd-4337-dcf4-2e82c16cc097"
      },
      "source": [
        "print(\"The accuracy of DefaultTagger is %.3f\" % default_tagger.evaluate(test_sents) )\n",
        "print(\"The accuracy of UnigramTagger (trained with the top-100 most frequent word tags) is %.3f\" % unigram_tagger_top100.evaluate(test_sents))\n",
        "print(\"The accuracy of UnigramTagger (trained with the top-1000 most frequent word tags) is %.3f\" % unigram_tagger_top1000.evaluate(test_sents))\n",
        "print(\"The accuracy of UnigramTagger (trained with all tagged training data) is %.3f\" % unigram_tagger.evaluate(test_sents))\n",
        "print(\"The accuracy of BigramTagger is %.3f\" % t2.evaluate(test_sents))\n",
        "print(\"The accuracy of BigramTagger (with UnigramTagger as backoff) is %.3f\" % t2_t1.evaluate(test_sents))\n",
        "print(\"The accuracy of BigramTagger (with DefaultTagger as backoff) is %.3f\" % t2_default.evaluate(test_sents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of DefaultTagger is 0.118\n",
            "The accuracy of UnigramTagger (trained with the top-100 most frequent word tags) is 0.491\n",
            "The accuracy of UnigramTagger (trained with the top-1000 most frequent word tags) is 0.640\n",
            "The accuracy of UnigramTagger (trained with all tagged training data) is 0.795\n",
            "The accuracy of BigramTagger is 0.117\n",
            "The accuracy of BigramTagger (with UnigramTagger as backoff) is 0.806\n",
            "The accuracy of BigramTagger (with DefaultTagger as backoff) is 0.725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaKnfcTY5Rho"
      },
      "source": [
        "--------------------------------------------This is the end of HW_2----------------------------"
      ]
    }
  ]
}
